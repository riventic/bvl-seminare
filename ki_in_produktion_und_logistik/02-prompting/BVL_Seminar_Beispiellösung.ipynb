{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEplMNxyKoAm",
        "outputId": "299387c2-501b-4868-d526-6708ab029c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schritt 1: Daten laden\n",
            "Daten aus 'delivery_data.csv' erfolgreich geladen.\n",
            "\n",
            "--- Erste 5 Zeilen der Daten (Validierung) ---\n",
            "   order_id   material_type  quantity supplier_location destination_location  \\\n",
            "0         1  Fertigprodukte       839          Mannheim              Leipzig   \n",
            "1         2      Elektronik       782             Paris  M√ºlheim an der Ruhr   \n",
            "2         3        Bauteile        52        Oberhausen               Erfurt   \n",
            "3         4       Rohstoffe       650     Gelsenkirchen                Mainz   \n",
            "4         5       Rohstoffe       839   M√∂nchengladbach             N√ºrnberg   \n",
            "\n",
            "   order_date weather_conditions  holiday_season  distance_km transport_mode  \\\n",
            "0  2023-06-20               Klar           False       344.98        Schiene   \n",
            "1  2023-05-14         Regnerisch           False       431.50         Stra√üe   \n",
            "2  2023-12-23              Sturm           False       294.96         Stra√üe   \n",
            "3  2023-09-26              Nebel           False       187.69         Stra√üe   \n",
            "4  2023-11-26         Regnerisch           False       382.03           Luft   \n",
            "\n",
            "         route_type  delivery_time_days  \n",
            "0  domestic_germany                 3.0  \n",
            "1    intra_european                 4.6  \n",
            "2  domestic_germany                 2.4  \n",
            "3  domestic_germany                 5.7  \n",
            "4  domestic_germany                 4.4  \n",
            "\n",
            "--- Daten-Typen und fehlende Werte (Validierung) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15000 entries, 0 to 14999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   order_id              15000 non-null  int64  \n",
            " 1   material_type         15000 non-null  object \n",
            " 2   quantity              15000 non-null  int64  \n",
            " 3   supplier_location     15000 non-null  object \n",
            " 4   destination_location  15000 non-null  object \n",
            " 5   order_date            15000 non-null  object \n",
            " 6   weather_conditions    15000 non-null  object \n",
            " 7   holiday_season        15000 non-null  bool   \n",
            " 8   distance_km           15000 non-null  float64\n",
            " 9   transport_mode        15000 non-null  object \n",
            " 10  route_type            15000 non-null  object \n",
            " 11  delivery_time_days    15000 non-null  float64\n",
            "dtypes: bool(1), float64(2), int64(2), object(7)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Schritt 1: Daten laden\n",
        "print(\"Schritt 1: Daten laden\")\n",
        "\n",
        "# Dateinamen definieren\n",
        "file_name = 'delivery_data.csv'\n",
        "\n",
        "try:\n",
        "    # CSV-Datei laden, Semikolon als Trennzeichen verwenden\n",
        "    df = pd.read_csv(file_name, delimiter=';')\n",
        "\n",
        "    # Validierung: √úberpr√ºfen, ob die Daten geladen wurden\n",
        "    print(f\"Daten aus '{file_name}' erfolgreich geladen.\")\n",
        "    print(\"\\n--- Erste 5 Zeilen der Daten (Validierung) ---\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\n--- Daten-Typen und fehlende Werte (Validierung) ---\")\n",
        "    df.info()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Fehler: Die Datei '{file_name}' wurde nicht gefunden.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ein Fehler ist aufgetreten: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Schritt 2: Daten analysieren\")\n",
        "\n",
        "# Daten laden (erneut, da Colab-Zellen-Kontext)\n",
        "try:\n",
        "    df = pd.read_csv('delivery_data.csv', delimiter=';')\n",
        "except Exception as e:\n",
        "    print(f\"Fehler beim Laden der Daten: {e}\")\n",
        "    # Hier w√ºrden wir normalerweise abbrechen, wenn die Daten nicht geladen werden k√∂nnen\n",
        "    raise e\n",
        "\n",
        "# --- 1. Datentyp 'order_date' korrigieren ---\n",
        "# (Wie in Schritt 1 bei df.info() gesehen, war 'order_date' ein 'object')\n",
        "print(\"\\n--- 1. Datentyp-Korrektur ('order_date') ---\")\n",
        "try:\n",
        "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
        "    print(\"Datentyp 'order_date' erfolgreich in 'datetime' umgewandelt.\")\n",
        "except Exception as e:\n",
        "    print(f\"Fehler bei der Datumsumwandlung: {e}\")\n",
        "\n",
        "# --- 2. Statistische Kennzahlen f√ºr NUMERISCHE Merkmale ---\n",
        "# (Wie vom Nutzer gefordert: Mittelwert, Std, Min, Max)\n",
        "print(\"\\n--- 2. Statistische Kennzahlen (Numerische Merkmale) ---\")\n",
        "# Wir nutzen .describe() f√ºr alle numerischen Typen\n",
        "# 'order_id' ist zwar numerisch, aber nur ein Identifikator.\n",
        "# Wir filtern die relevanten Spalten, die der Nutzer im Prompt genannt hat + Zielvariable\n",
        "numerical_features = ['quantity', 'distance_km', 'delivery_time_days']\n",
        "print(df[numerical_features].describe())\n",
        "\n",
        "# --- 3. Analyse der KATEGORISCHEN Merkmale ---\n",
        "print(\"\\n--- 3. Analyse der Kategorischen Merkmale (inkl. Boolean) ---\")\n",
        "# 'include=['object', 'bool']' zeigt Statistiken f√ºr nicht-numerische Daten\n",
        "# (Anzahl, Eindeutige Werte, H√§ufigster Wert, H√§ufigkeit)\n",
        "try:\n",
        "    print(df.describe(include=['object', 'bool']))\n",
        "except Exception as e:\n",
        "    print(f\"Fehler bei der Analyse der kategorischen Merkmale: {e}\")\n",
        "\n",
        "print(\"\\n--- Validierung der Datentypen nach Korrektur ---\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6sqPgs4OEUu",
        "outputId": "a479d8cb-b4da-48b1-c028-1f2a58e16a47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schritt 2: Daten analysieren\n",
            "\n",
            "--- 1. Datentyp-Korrektur ('order_date') ---\n",
            "Datentyp 'order_date' erfolgreich in 'datetime' umgewandelt.\n",
            "\n",
            "--- 2. Statistische Kennzahlen (Numerische Merkmale) ---\n",
            "           quantity   distance_km  delivery_time_days\n",
            "count  15000.000000  15000.000000        15000.000000\n",
            "mean     498.581067   4651.110221           21.823400\n",
            "std      288.411239   5127.007721           26.116256\n",
            "min        1.000000      4.240000            1.000000\n",
            "25%      249.000000    310.840000            4.600000\n",
            "50%      497.000000    997.020000            9.900000\n",
            "75%      746.000000   8872.020000           33.800000\n",
            "max      999.000000  18758.880000          367.700000\n",
            "\n",
            "--- 3. Analyse der Kategorischen Merkmale (inkl. Boolean) ---\n",
            "       material_type supplier_location destination_location  \\\n",
            "count          15000             15000                15000   \n",
            "unique             5                74                   74   \n",
            "top       Elektronik            Berlin               Mumbai   \n",
            "freq            3033               242                  229   \n",
            "\n",
            "       weather_conditions holiday_season transport_mode        route_type  \n",
            "count               15000          15000          15000             15000  \n",
            "unique                  5              2              4                 3  \n",
            "top            Regnerisch          False           Luft  intercontinental  \n",
            "freq                 3068          13542           8860              7459  \n",
            "\n",
            "--- Validierung der Datentypen nach Korrektur ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15000 entries, 0 to 14999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                Non-Null Count  Dtype         \n",
            "---  ------                --------------  -----         \n",
            " 0   order_id              15000 non-null  int64         \n",
            " 1   material_type         15000 non-null  object        \n",
            " 2   quantity              15000 non-null  int64         \n",
            " 3   supplier_location     15000 non-null  object        \n",
            " 4   destination_location  15000 non-null  object        \n",
            " 5   order_date            15000 non-null  datetime64[ns]\n",
            " 6   weather_conditions    15000 non-null  object        \n",
            " 7   holiday_season        15000 non-null  bool          \n",
            " 8   distance_km           15000 non-null  float64       \n",
            " 9   transport_mode        15000 non-null  object        \n",
            " 10  route_type            15000 non-null  object        \n",
            " 11  delivery_time_days    15000 non-null  float64       \n",
            "dtypes: bool(1), datetime64[ns](1), float64(2), int64(2), object(6)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "print(\"Schritt 3: Daten visualisieren\")\n",
        "\n",
        "# --- 1. Daten laden ---\n",
        "try:\n",
        "    df = pd.read_csv('delivery_data.csv', delimiter=';')\n",
        "except Exception as e:\n",
        "    print(f\"Fehler beim Laden der Daten: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- 2. Datenbereinigung (NaN) ---\n",
        "# Fokussieren auf die relevanten Spalten\n",
        "relevant_cols = ['delivery_time_days', 'route_type', 'weather_conditions']\n",
        "df_cleaned = df[relevant_cols].copy()\n",
        "\n",
        "# √úberpr√ºfen auf NaN (obwohl wir von Schritt 1 wissen, dass es keine gibt,\n",
        "# setzen wir die Anforderung um)\n",
        "initial_rows = len(df_cleaned)\n",
        "df_cleaned.dropna(inplace=True)\n",
        "cleaned_rows = len(df_cleaned)\n",
        "\n",
        "if initial_rows > cleaned_rows:\n",
        "    print(f\"{initial_rows - cleaned_rows} Zeilen mit NaN-Werten entfernt.\")\n",
        "else:\n",
        "    print(\"Keine NaN-Werte in den relevanten Spalten gefunden. (Gute Praxis)\")\n",
        "\n",
        "# --- 3. & 4. Datenaggregation und Aufbereitung ---\n",
        "# Berechne die durchschnittliche Transportzeit pro Routentyp und Wetter\n",
        "try:\n",
        "    df_agg = df_cleaned.groupby(['route_type', 'weather_conditions'])['delivery_time_days'].mean().reset_index()\n",
        "    # Runden des Mittelwerts f√ºr eine sauberere Anzeige im Tooltip\n",
        "    df_agg['delivery_time_days'] = round(df_agg['delivery_time_days'], 2)\n",
        "    print(\"\\nDaten erfolgreich aggregiert (Durchschnittliche Transportzeit):\")\n",
        "    print(df_agg.head())\n",
        "except Exception as e:\n",
        "    print(f\"Fehler bei der Datenaggregation: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- 5. Diagramm-Erstellung (Gruppiertes S√§ulendiagramm) ---\n",
        "print(\"\\nErstelle gruppiertes S√§ulendiagramm...\")\n",
        "try:\n",
        "    chart = alt.Chart(df_agg).mark_bar().encode(\n",
        "        # X-Achse (innerhalb der Gruppe): Wetter. Achsentitel entfernen.\n",
        "        x=alt.X('weather_conditions', axis=None),\n",
        "\n",
        "        # Y-Achse: Durchschnittliche Lieferzeit\n",
        "        y=alt.Y('delivery_time_days', title='Durchschnittl. Transportzeit (Tage)'),\n",
        "\n",
        "        # Farbe: Wird auch f√ºr die Gruppierung (Wetter) verwendet\n",
        "        color=alt.Color('weather_conditions', title='Wetter'),\n",
        "\n",
        "        # Spalten (Hauptgruppierung): Routentyp.\n",
        "        # Titel und Labels nach unten verschoben f√ºr bessere Lesbarkeit.\n",
        "        column=alt.Column(\n",
        "            'route_type',\n",
        "            title='Routentyp',\n",
        "            header=alt.Header(titleOrient=\"bottom\", labelOrient=\"bottom\")\n",
        "        ),\n",
        "\n",
        "        # Tooltip f√ºr Interaktivit√§t\n",
        "        tooltip=['route_type', 'weather_conditions', 'delivery_time_days']\n",
        "    ).interactive()\n",
        "\n",
        "    # --- 6. Speichern ---\n",
        "    chart_path = 'delivery_time_by_route_weather.json'\n",
        "    chart.save(chart_path)\n",
        "    print(f\"Diagramm erfolgreich als '{chart_path}' gespeichert.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Fehler bei der Diagrammerstellung mit Altair: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFpqUwiOOesm",
        "outputId": "53cea49b-96b9-48bc-c458-c5213d8a2eb2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schritt 3: Daten visualisieren\n",
            "Keine NaN-Werte in den relevanten Spalten gefunden. (Gute Praxis)\n",
            "\n",
            "Daten erfolgreich aggregiert (Durchschnittliche Transportzeit):\n",
            "         route_type weather_conditions  delivery_time_days\n",
            "0  domestic_germany               Klar                4.11\n",
            "1  domestic_germany              Nebel                4.27\n",
            "2  domestic_germany         Regnerisch                4.04\n",
            "3  domestic_germany             Schnee                4.57\n",
            "4  domestic_germany              Sturm                4.71\n",
            "\n",
            "Erstelle gruppiertes S√§ulendiagramm...\n",
            "Diagramm erfolgreich als 'delivery_time_by_route_weather.json' gespeichert.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "print(\"Schritt 4: KI-Modell (Regression) trainieren\")\n",
        "\n",
        "# --- 1. Daten laden und vorbereiten ---\n",
        "try:\n",
        "    df = pd.read_csv('delivery_data.csv', delimiter=';')\n",
        "except Exception as e:\n",
        "    print(f\"Fehler beim Laden der Daten: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- Feature Engineering (Schritt 2 der Checkliste) ---\n",
        "# Wir wandeln 'order_date' in n√ºtzlichere numerische Features um.\n",
        "# Das Modell kann mit \"Monat\" und \"Wochentag\" oft mehr anfangen.\n",
        "try:\n",
        "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
        "    df['order_month'] = df['order_date'].dt.month\n",
        "    df['order_day_of_week'] = df['order_date'].dt.dayofweek\n",
        "    # Die urspr√ºngliche Datumsspalte wird nicht mehr ben√∂tigt\n",
        "    df = df.drop('order_date', axis=1)\n",
        "    print(\"Feature Engineering f√ºr 'order_date' abgeschlossen.\")\n",
        "except Exception as e:\n",
        "    print(f\"Fehler beim Feature Engineering: {e}\")\n",
        "\n",
        "# --- 2. & 4. & 5. Preprocessing und Pipeline definieren ---\n",
        "\n",
        "# Definition der Spalten (Features)\n",
        "# HINWEIS: 'order_date' ist jetzt 'order_month' und 'order_day_of_week'\n",
        "# Diese 11 Features entsprechen den 10 urspr√ºnglichen Eingabefeldern\n",
        "numeric_features = ['quantity', 'distance_km']\n",
        "categorical_features = [\n",
        "    'material_type', 'supplier_location', 'destination_location',\n",
        "    'transport_mode', 'route_type', 'weather_conditions',\n",
        "    'order_month', 'order_day_of_week' # Behandeln wir als kategorial\n",
        "]\n",
        "boolean_feature = ['holiday_season'] # 'holiday_season'\n",
        "\n",
        "# Definition der Zielvariable (Label)\n",
        "label = 'delivery_time_days'\n",
        "\n",
        "# Erstellen der Preprocessing-Pipelines f√ºr verschiedene Datentypen\n",
        "# Numerisch: Skalieren (Mittelwert 0, Varianz 1)\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Kategorial: One-Hot-Encoding (Erzeugt Dummy-Variablen)\n",
        "# handle_unknown='ignore' sorgt daf√ºr, dass das Modell bei neuen,\n",
        "# im Training unbekannten Werten (z.B. neue Stadt) keinen Fehler wirft.\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Zusammenbau des Preprocessors mit ColumnTransformer\n",
        "# ('bool', 'passthrough', boolean_feature) -> 'holiday_season' (True/False)\n",
        "# wird unver√§ndert (als 0/1) durchgelassen.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('bool', 'passthrough', boolean_feature)\n",
        "    ],\n",
        "    remainder='drop' # Ignoriert andere Spalten (wie order_id)\n",
        ")\n",
        "\n",
        "# Definition des Modells\n",
        "regressor_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Erstellen der vollst√§ndigen Pipeline (Preprocessing + Modell)\n",
        "# Dies ist das Artefakt, das wir in Schritt 5 speichern werden.\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', regressor_model)\n",
        "])\n",
        "\n",
        "print(\"ML-Pipeline erfolgreich definiert.\")\n",
        "\n",
        "# --- 3. Datentrennung ---\n",
        "# Wir definieren X (alle Features) und y (das Label)\n",
        "# Wir verwenden die 10 urspr√ºnglichen Features + die 2 neuen Datumsfeatures\n",
        "all_features = numeric_features + categorical_features + boolean_feature\n",
        "X = df[all_features]\n",
        "y = df[label]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Daten gesplittet: {len(X_train)} Trainingsdatens√§tze, {len(X_test)} Testdatens√§tze.\")\n",
        "\n",
        "# --- 5. Modelltraining ---\n",
        "print(\"Starte Modelltraining...\")\n",
        "try:\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "    print(\"Modelltraining erfolgreich abgeschlossen.\")\n",
        "except Exception as e:\n",
        "    print(f\"Fehler w√§hrend des Trainings: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- 6. Modell-Evaluierung ---\n",
        "print(\"\\n--- Modell-Evaluierung ---\")\n",
        "# Vorhersage auf den Testdaten\n",
        "y_pred = model_pipeline.predict(X_test)\n",
        "\n",
        "# Berechnung der Metriken\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"R-squared (R¬≤) Score: {r2:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f} (Tage)\")\n",
        "\n",
        "# Validierung basierend auf R¬≤\n",
        "if r2 > 0.7:\n",
        "    print(\"Validierung: Modellgenauigkeit (R¬≤ > 0.7) ist gut.\")\n",
        "elif r2 > 0.5:\n",
        "    print(\"Validierung: Modellgenauigkeit (R¬≤ > 0.5) ist akzeptabel, k√∂nnte aber besser sein.\")\n",
        "else:\n",
        "    print(\"Validierung: Modellgenauigkeit (R¬≤ < 0.5) ist niedrig. Ggf. mehr Feature Engineering n√∂tig.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4TMCTVzP64B",
        "outputId": "4b296223-e992-4b8a-ac83-2655a8a20012"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schritt 4: KI-Modell (Regression) trainieren\n",
            "Feature Engineering f√ºr 'order_date' abgeschlossen.\n",
            "ML-Pipeline erfolgreich definiert.\n",
            "Daten gesplittet: 12000 Trainingsdatens√§tze, 3000 Testdatens√§tze.\n",
            "Starte Modelltraining...\n",
            "Modelltraining erfolgreich abgeschlossen.\n",
            "\n",
            "--- Modell-Evaluierung ---\n",
            "R-squared (R¬≤) Score: 0.9804\n",
            "Root Mean Squared Error (RMSE): 3.8176 (Tage)\n",
            "Validierung: Modellgenauigkeit (R¬≤ > 0.7) ist gut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "print(\"Schritt 5: Modell speichern\")\n",
        "\n",
        "# --- 1. Kontext und Training (Vorbereitung zum Speichern) ---\n",
        "# Damit diese Zelle robust ist, definieren wir die Pipeline neu\n",
        "# und trainieren sie mit 100% der Daten (Best Practice).\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('delivery_data.csv', delimiter=';')\n",
        "except Exception as e:\n",
        "    print(f\"Fehler beim Laden der Daten: {e}\")\n",
        "    raise e\n",
        "\n",
        "# 1a. Feature Engineering (wie in Schritt 4)\n",
        "try:\n",
        "    df['order_date'] = pd.to_datetime(df['order_date'])\n",
        "    df['order_month'] = df['order_date'].dt.month\n",
        "    df['order_day_of_week'] = df['order_date'].dt.dayofweek\n",
        "    df = df.drop('order_date', axis=1)\n",
        "except Exception as e:\n",
        "    print(f\"Fehler beim Feature Engineering: {e}\")\n",
        "\n",
        "# 1b. Definition der Features und Pipeline (wie in Schritt 4)\n",
        "numeric_features = ['quantity', 'distance_km']\n",
        "categorical_features = [\n",
        "    'material_type', 'supplier_location', 'destination_location',\n",
        "    'transport_mode', 'route_type', 'weather_conditions',\n",
        "    'order_month', 'order_day_of_week'\n",
        "]\n",
        "boolean_feature = ['holiday_season']\n",
        "label = 'delivery_time_days'\n",
        "\n",
        "all_features = numeric_features + categorical_features + boolean_feature\n",
        "\n",
        "# Pipeline-Definition\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('bool', 'passthrough', boolean_feature)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "regressor_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', regressor_model)\n",
        "])\n",
        "\n",
        "# 1c. Training mit 100% der Daten\n",
        "print(\"Trainiere das finale Modell mit 100% der Daten (Best Practice)...\")\n",
        "try:\n",
        "    X_full = df[all_features]\n",
        "    y_full = df[label]\n",
        "\n",
        "    model_pipeline.fit(X_full, y_full)\n",
        "    print(\"Training des finalen Modells abgeschlossen.\")\n",
        "except Exception as e:\n",
        "    print(f\"Fehler w√§hrend des finalen Trainings: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- 2. Modell speichern ---\n",
        "model_filename = 'transport_model.joblib'\n",
        "try:\n",
        "    # Speichern der gesamten Pipeline (Preprocessor + Regressor)\n",
        "    joblib.dump(model_pipeline, model_filename)\n",
        "\n",
        "    # Validierung: Pr√ºfen, ob die Datei existiert\n",
        "    import os\n",
        "    if os.path.exists(model_filename):\n",
        "        print(f\"\\nValidierung: Pipeline erfolgreich als '{model_filename}' gespeichert.\")\n",
        "    else:\n",
        "        print(f\"\\nFehler: Datei '{model_filename}' wurde nicht erstellt.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Fehler beim Speichern des Modells: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAoSn4eYQ-mD",
        "outputId": "397abd6f-82cb-41d0-eb43-0d1eda2c1c9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schritt 5: Modell speichern\n",
            "Trainiere das finale Modell mit 100% der Daten (Best Practice)...\n",
            "Training des finalen Modells abgeschlossen.\n",
            "\n",
            "Validierung: Pipeline erfolgreich als 'transport_model.joblib' gespeichert.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Schritt 6: Benutzeroberfl√§che (Gradio) ---\n",
        "# HINWEIS: Dieser Code muss in einer Umgebung ausgef√ºhrt werden,\n",
        "# in der 'gradio' installiert ist (z.B. nach 'pip install gradio').\n",
        "\n",
        "print(\"Schritt 6: Benutzeroberfl√§che entwickeln und mit Gradio hosten\")\n",
        "\n",
        "# --- 1. Imports ---\n",
        "try:\n",
        "    import gradio as gr\n",
        "except ImportError:\n",
        "    print(\"Fehler: 'gradio' ist nicht installiert.\")\n",
        "    print(\"Bitte f√ºhren Sie 'pip install gradio' aus und starten Sie das Skript erneut.\")\n",
        "    exit()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "print(\"Gradio und notwendige Bibliotheken importiert.\")\n",
        "\n",
        "# --- 2. Modell laden (Einmalig beim Start) ---\n",
        "MODEL_FILE = 'transport_model.joblib'\n",
        "MODEL_PIPELINE = None\n",
        "try:\n",
        "    MODEL_PIPELINE = joblib.load(MODEL_FILE)\n",
        "    print(f\"Modell '{MODEL_FILE}' erfolgreich geladen.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"FATALER FEHLER: Modelldatei '{MODEL_FILE}' nicht gefunden.\")\n",
        "    print(\"Bitte Schritt 5 (Modell speichern) zuerst ausf√ºhren.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATALER FEHLER: Modell konnte nicht geladen werden: {e}\")\n",
        "\n",
        "# Definition der Spalten, die das Modell (und die Validierung) erwartet\n",
        "EXPECTED_COLUMNS = [\n",
        "    \"material_type\", \"quantity\", \"supplier_location\", \"destination_location\",\n",
        "    \"distance_km\", \"transport_mode\", \"route_type\", \"order_date\",\n",
        "    \"weather_conditions\", \"holiday_season\"\n",
        "]\n",
        "\n",
        "# --- 3. Kernlogik (Die Vorhersage-Funktion) ---\n",
        "def predict_batch_csv(uploaded_file):\n",
        "    \"\"\"\n",
        "    Wird von Gradio aufgerufen. Validiert, verarbeitet und sagt\n",
        "    die Transportzeiten f√ºr eine hochgeladene CSV-Datei voraus.\n",
        "    \"\"\"\n",
        "\n",
        "    # Validierung 1: Modell geladen?\n",
        "    if MODEL_PIPELINE is None:\n",
        "        return None, None, \"FEHLER: Das KI-Modell ist nicht geladen. App bitte neu starten.\"\n",
        "\n",
        "    # Validierung 2: Datei hochgeladen?\n",
        "    if uploaded_file is None:\n",
        "        return None, None, \"Fehler: Bitte eine CSV-Datei hochladen.\"\n",
        "\n",
        "    # Pfad zur tempor√§ren Datei (Gradio 3+)\n",
        "    temp_file_path = uploaded_file.name\n",
        "\n",
        "    try:\n",
        "        # 1. Daten laden (Robustes Parsen)\n",
        "        df = None\n",
        "        try:\n",
        "            # Versuche Komma\n",
        "            df = pd.read_csv(temp_file_path)\n",
        "        except (pd.errors.ParserError, UnicodeDecodeError, IsADirectoryError):\n",
        "            df = None # Setze zur√ºck, falls erster Versuch fehlschl√§gt\n",
        "\n",
        "        if df is None or len(df.columns) <= 1:\n",
        "             try:\n",
        "                # Versuche Semikolon (wie die Originaldatei)\n",
        "                df = pd.read_csv(temp_file_path, delimiter=';')\n",
        "             except Exception as e_inner:\n",
        "                return None, None, f\"CSV-Lesefehler. Bitte Komma (,) oder Semikolon (;) verwenden. Details: {e_inner}\"\n",
        "\n",
        "        if df is None:\n",
        "             return None, None, \"CSV-Datei konnte nicht gelesen werden.\"\n",
        "\n",
        "        df_input_original = df.copy()\n",
        "\n",
        "        # 2. Spaltenvalidierung\n",
        "        uploaded_cols_original = set(df.columns)\n",
        "        required_cols_set = set(EXPECTED_COLUMNS)\n",
        "\n",
        "        if not required_cols_set.issubset(uploaded_cols_original):\n",
        "            missing = list(required_cols_set - uploaded_cols_original)\n",
        "            return None, None, f\"Fehler: Fehlende Spalten: {missing}. Gefunden: {list(uploaded_cols_original)}\"\n",
        "\n",
        "        df = df[EXPECTED_COLUMNS]\n",
        "\n",
        "        # 3. Datentyp-Validierung & Konvertierung\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        try:\n",
        "            df_processed['order_date'] = pd.to_datetime(df_processed['order_date'])\n",
        "        except Exception:\n",
        "            return None, None, \"Fehler: Spalte 'order_date' (z.B. '2024-07-05') konnte nicht verarbeitet werden.\"\n",
        "\n",
        "        try:\n",
        "            df_processed['quantity'] = pd.to_numeric(df_processed['quantity'])\n",
        "        except Exception:\n",
        "            return None, None, \"Fehler: Spalte 'quantity' muss numerisch sein.\"\n",
        "\n",
        "        try:\n",
        "            df_processed['distance_km'] = pd.to_numeric(df_processed['distance_km'])\n",
        "        except Exception:\n",
        "            return None, None, \"Fehler: Spalte 'distance_km' muss numerisch sein.\"\n",
        "\n",
        "        if df_processed['holiday_season'].dtype == 'object':\n",
        "            df_processed['holiday_season'] = df_processed['holiday_season'].str.lower().str.strip().map({\n",
        "                'true': True, 'false': False, '1': True, '0': False,\n",
        "                'wahr': True, 'falsch': False, 'ja': True, 'nein': False\n",
        "            }).fillna(False)\n",
        "\n",
        "        df_processed['holiday_season'] = df_processed['holiday_season'].astype(bool)\n",
        "\n",
        "        print(\"Validierung der Eingabedaten erfolgreich.\")\n",
        "\n",
        "        # 4. Feature Engineering\n",
        "        df_processed['order_month'] = df_processed['order_date'].dt.month\n",
        "        df_processed['order_day_of_week'] = df_processed['order_date'].dt.dayofweek\n",
        "        print(\"Feature Engineering abgeschlossen.\")\n",
        "\n",
        "        # 5. Vorhersage\n",
        "        predictions = MODEL_PIPELINE.predict(df_processed)\n",
        "        print(\"Vorhersage erfolgreich durchgef√ºhrt.\")\n",
        "\n",
        "        # 6. Ergebnisse formatieren\n",
        "        df_input_original['VORHERSAGE_Transportzeit_Tage'] = np.round(predictions, 2)\n",
        "\n",
        "        # 7. Download-Datei erstellen\n",
        "        download_path = \"ergebnisse_transportvorhersage.csv\"\n",
        "        df_input_original.to_csv(download_path, index=False, sep=';', encoding='utf-8-sig')\n",
        "\n",
        "        return df_input_original, download_path, \"Vorhersage erfolgreich abgeschlossen.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        if temp_file_path and os.path.exists(temp_file_path):\n",
        "             os.remove(temp_file_path)\n",
        "        print(f\"Ein Fehler ist aufgetreten: {e}\")\n",
        "        return None, None, f\"Allgemeiner Fehler: {str(e)}\"\n",
        "    finally:\n",
        "        # Aufr√§umen: L√∂sche die tempor√§re Upload-Datei\n",
        "        if temp_file_path and os.path.exists(temp_file_path) and os.path.isfile(temp_file_path):\n",
        "             try:\n",
        "                 os.remove(temp_file_path)\n",
        "             except OSError as e_rm:\n",
        "                 print(f\"Fehler beim L√∂schen der tempor√§ren Datei {temp_file_path}: {e_rm}\")\n",
        "\n",
        "\n",
        "# --- 4. UI-Definition (Gradio Blocks) ---\n",
        "csv_format_description = \"\"\"\n",
        "**Anleitung und CSV-Format:**\n",
        "\n",
        "1.  Laden Sie eine CSV-Datei hoch (Trennzeichen: Komma `,` oder Semikolon `;`).\n",
        "2.  Die Datei *muss* die folgenden 10 Spalten enthalten (Reihenfolge egal, Namen m√ºssen exakt stimmen):\n",
        "    - `material_type` (Text, z.B. \"Elektronik\")\n",
        "    - `quantity` (Zahl, z.B. 1000)\n",
        "    - `supplier_location` (Text, z.B. \"Berlin\")\n",
        "    - `destination_location` (Text, z.B. \"Hamburg\")\n",
        "    - `distance_km` (Zahl, z.B. 280.0)\n",
        "    - `transport_mode` (Text, z.B. \"Stra√üe\")\n",
        "    - `route_type` (Text, z.B. \"domestic_germany\")\n",
        "    - `order_date` (Datum/Text, z.B. \"2024-07-05\")\n",
        "    - `weather_conditions` (Text, z.B. \"Regnerisch\")\n",
        "    - `holiday_season` (Boolean, z.B. `TRUE`/`FALSE`, `1`/`0`, `wahr`/`falsch`)\n",
        "3.  Klicken Sie auf \"Vorhersage starten\".\n",
        "\"\"\"\n",
        "\n",
        "# √úberpr√ºfen, ob das Modell geladen wurde, bevor die UI definiert wird\n",
        "if MODEL_PIPELINE is not None:\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as iface:\n",
        "        gr.Markdown(\"# KI-Modell: Vorhersage von Transportzeiten üöö\")\n",
        "        gr.Markdown(csv_format_description)\n",
        "\n",
        "        with gr.Row():\n",
        "            csv_in = gr.File(label=\"CSV-Datei hochladen\", file_types=[\".csv\"])\n",
        "            btn_predict = gr.Button(\"Vorhersage starten\", variant=\"primary\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "        gr.Markdown(\"### Ergebnisse\")\n",
        "\n",
        "        status_text = gr.Textbox(label=\"Status / Fehler\", interactive=False)\n",
        "        results_table = gr.Dataframe(label=\"Ergebnisse (Eingabe + Vorhersage)\", interactive=False)\n",
        "        results_file = gr.File(label=\"Ergebnis-CSV herunterladen\")\n",
        "\n",
        "        btn_predict.click(\n",
        "            fn=predict_batch_csv,\n",
        "            inputs=[csv_in],\n",
        "            outputs=[results_table, results_file, status_text]\n",
        "        )\n",
        "\n",
        "# --- 5. App starten ---\n",
        "if MODEL_PIPELINE is not None and 'iface' in locals():\n",
        "    print(\"\\n--- Gradio App wird gestartet ---\")\n",
        "    print(\"Klicken Sie auf den Link (z.B. [http://127.0.0.1:7860](http://127.0.0.1:7860)) oder den Public Link (share=True).\")\n",
        "\n",
        "    # share=True erstellt einen √∂ffentlichen Link (n√ºtzlich in Colab)\n",
        "    # debug=True zeigt detailliertere Fehler in der Konsole an\n",
        "    iface.launch(share=True, debug=True)\n",
        "else:\n",
        "    print(\"\\n--- Gradio App NICHT gestartet ---\")\n",
        "    print(\"Das Modell konnte nicht geladen werden. Bitte beheben Sie den Fehler und f√ºhren Sie das Skript erneut aus.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AwtIstf3Rvqe",
        "outputId": "998b17db-9cb4-4938-ae8f-e3ee61c73886"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schritt 6: Benutzeroberfl√§che entwickeln und mit Gradio hosten\n",
            "Gradio und notwendige Bibliotheken importiert.\n",
            "Modell 'transport_model.joblib' erfolgreich geladen.\n",
            "\n",
            "--- Gradio App wird gestartet ---\n",
            "Klicken Sie auf den Link (z.B. [http://127.0.0.1:7860](http://127.0.0.1:7860)) oder den Public Link (share=True).\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://14c599098c54361648.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://14c599098c54361648.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1134, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 125, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 111, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 391, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 290, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x78a16b34e210 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validierung der Eingabedaten erfolgreich.\n",
            "Feature Engineering abgeschlossen.\n",
            "Vorhersage erfolgreich durchgef√ºhrt.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://14c599098c54361648.gradio.live\n"
          ]
        }
      ]
    }
  ]
}